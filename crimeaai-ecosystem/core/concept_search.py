"""
Concept Search - –ü–æ–∏—Å–∫ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
===========================================

–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ —á–µ—Ä–µ–∑ DuckDuckGo –∏ –∏—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
–≤ –ø–∞–º—è—Ç—å —Å–∏—Å—Ç–µ–º—ã.
"""

import re
import time
import asyncio
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Set, Tuple
from collections import Counter
import random


# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ requests –∏ BeautifulSoup
try:
    import requests
    from bs4 import BeautifulSoup
    SCRAPING_AVAILABLE = True
except ImportError:
    SCRAPING_AVAILABLE = False
    print("‚ö†Ô∏è requests/beautifulsoup4 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –ü–æ–∏—Å–∫ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –±—É–¥–µ—Ç —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω.")


@dataclass
class Concept:
    """–ö–æ–Ω—Ü–µ–ø—Ç - –µ–¥–∏–Ω–∏—Ü–∞ –∑–Ω–∞–Ω–∏—è"""
    term: str                              # –¢–µ—Ä–º–∏–Ω
    definition: str = ""                   # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
    source_url: str = ""                   # –ò—Å—Ç–æ—á–Ω–∏–∫
    related_terms: List[str] = field(default_factory=list)  # –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
    embedding: Optional[List[float]] = None  # –í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
    importance: float = 1.0                # –í–∞–∂–Ω–æ—Å—Ç—å
    discovery_time: float = 0.0            # –í—Ä–µ–º—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è
    access_count: int = 0                  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞—â–µ–Ω–∏–π
    
    def to_dict(self) -> dict:
        return {
            'term': self.term,
            'definition': self.definition,
            'source_url': self.source_url,
            'related_terms': self.related_terms,
            'importance': self.importance,
            'discovery_time': self.discovery_time
        }


class ConceptExtractor:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤
    TERM_PATTERNS = [
        r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)+\b',  # CamelCase phrases
        r'\b[A-Z]{2,}\b',                         # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        r'\b\w+(?:tion|ment|ness|ity|ism)\b',    # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
    ]
    
    # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
    STOP_WORDS = {
        'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been',
        'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',
        'could', 'should', 'may', 'might', 'must', 'shall', 'can',
        'this', 'that', 'these', 'those', 'it', 'its', 'with', 'for',
        'from', 'into', 'onto', 'upon', 'about', 'above', 'below',
        'between', 'under', 'over', 'through', 'during', 'before',
        'after', 'while', 'where', 'when', 'what', 'which', 'who',
        'whom', 'whose', 'why', 'how', 'all', 'each', 'every', 'both',
        'few', 'more', 'most', 'other', 'some', 'such', 'than', 'too',
        'very', 'just', 'only', 'own', 'same', 'so', 'then', 'there'
    }
    
    def __init__(self, min_term_length: int = 3, max_term_length: int = 50):
        self.min_term_length = min_term_length
        self.max_term_length = max_term_length
    
    def extract_terms(self, text: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        """
        terms = []
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for pattern in self.TERM_PATTERNS:
            matches = re.findall(pattern, text)
            terms.extend(matches)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º n-–≥—Ä–∞–º–º—ã
        words = text.split()
        for i in range(len(words)):
            # –£–Ω–∏–≥—Ä–∞–º–º—ã
            word = words[i].strip('.,!?;:()[]{}"\'-')
            if self._is_valid_term(word):
                terms.append(word.lower())
            
            # –ë–∏–≥—Ä–∞–º–º—ã
            if i < len(words) - 1:
                bigram = f"{words[i]} {words[i+1]}".strip('.,!?;:()[]{}"\'-')
                if len(bigram.split()) == 2 and len(bigram) <= self.max_term_length:
                    terms.append(bigram.lower())
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        seen = set()
        unique_terms = []
        for term in terms:
            if term not in seen:
                seen.add(term)
                unique_terms.append(term)
        
        return unique_terms
    
    def _is_valid_term(self, term: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Ç–µ—Ä–º–∏–Ω–∞"""
        term_lower = term.lower()
        
        if len(term) < self.min_term_length:
            return False
        if len(term) > self.max_term_length:
            return False
        if term_lower in self.STOP_WORDS:
            return False
        if not any(c.isalpha() for c in term):
            return False
        
        return True
    
    def rank_terms(self, terms: List[str], context: str = "") -> List[Tuple[str, float]]:
        """
        –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        
        Args:
            terms: —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤
            context: –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ (—Ç–µ—Ä–º–∏–Ω, –≤–∞–∂–Ω–æ—Å—Ç—å)
        """
        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
        term_counts = Counter(terms)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å
        ranked = []
        for term, count in term_counts.items():
            importance = count / len(terms) if terms else 0
            
            # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω—É (–±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã —á–∞—Å—Ç–æ –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã)
            length_bonus = min(len(term) / 20, 0.5)
            
            # –ë–æ–Ω—É—Å –∑–∞ CamelCase –∏–ª–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
            if term[0].isupper():
                importance *= 1.5
            if term.isupper():
                importance *= 2.0
            
            # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            if context and term.lower() in context.lower():
                importance *= 1.2
            
            ranked.append((term, importance + length_bonus))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
        ranked.sort(key=lambda x: x[1], reverse=True)
        
        return ranked


class ConceptSearcher:
    """
    –ü–æ–∏—Å–∫–æ–≤–∏–∫ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ —á–µ—Ä–µ–∑ DuckDuckGo
    
    –û—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç –ø–æ–∏—Å–∫ –Ω–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π –∏ –∏—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –≤ —Å–∏—Å—Ç–µ–º—É.
    """
    
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
    ]
    
    def __init__(
        self,
        base_keywords: Optional[List[str]] = None,
        search_interval: float = 19 * 60  # 19 –º–∏–Ω—É—Ç
    ):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞
        
        Args:
            base_keywords: –±–∞–∑–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            search_interval: –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –ø–æ–∏—Å–∫–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
        """
        self.base_keywords = base_keywords or ['AI', 'machine learning', 'neural network']
        self.search_interval = search_interval
        
        self.extractor = ConceptExtractor()
        
        # –ë–∞–∑–∞ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
        self.concepts: Dict[str, Concept] = {}
        
        # –ò—Å—Ç–æ—Ä–∏—è –ø–æ–∏—Å–∫–æ–≤
        self.search_history: List[dict] = []
        self.last_search_time = 0.0
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_searches = 0
        self.total_concepts_found = 0
        self.failed_searches = 0
    
    def search_concepts(self, keywords: Optional[List[str]] = None) -> List[Concept]:
        """
        –ü–æ–∏—Å–∫ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        
        Args:
            keywords: –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±–∞–∑–æ–≤—ã–µ)
        
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤
        """
        if keywords is None:
            keywords = self.base_keywords
        
        self.total_searches += 1
        self.last_search_time = time.time()
        
        concepts = []
        
        if SCRAPING_AVAILABLE:
            concepts = self._search_duckduckgo(keywords)
        else:
            concepts = self._simulate_search(keywords)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.search_history.append({
            'time': self.last_search_time,
            'keywords': keywords,
            'concepts_found': len(concepts)
        })
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
        if len(self.search_history) > 100:
            self.search_history = self.search_history[-100:]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É
        for concept in concepts:
            self._add_concept(concept)
        
        self.total_concepts_found += len(concepts)
        
        return concepts
    
    def _search_duckduckgo(self, keywords: List[str]) -> List[Concept]:
        """–†–µ–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ DuckDuckGo"""
        concepts = []
        query = ' '.join(keywords)
        
        try:
            url = f"https://duckduckgo.com/html/?q={query}"
            headers = {
                'User-Agent': random.choice(self.USER_AGENTS),
                'Accept': 'text/html,application/xhtml+xml',
                'Accept-Language': 'en-US,en;q=0.9',
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            results = soup.find_all('a', class_='result__a')[:5]
            
            for result in results:
                link_url = result.get('href', '')
                title = result.get_text(strip=True)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                terms = self.extractor.extract_terms(title)
                ranked = self.extractor.rank_terms(terms, title)
                
                for term, importance in ranked[:3]:
                    concept = Concept(
                        term=term,
                        definition=title,
                        source_url=link_url,
                        importance=importance,
                        discovery_time=time.time()
                    )
                    concepts.append(concept)
                
                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                try:
                    page_response = requests.get(link_url, headers=headers, timeout=5)
                    if page_response.status_code == 200:
                        page_soup = BeautifulSoup(page_response.text, 'html.parser')
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                        text = page_soup.get_text()[:5000]
                        page_terms = self.extractor.extract_terms(text)
                        page_ranked = self.extractor.rank_terms(page_terms, text)
                        
                        for term, importance in page_ranked[:5]:
                            if term not in [c.term for c in concepts]:
                                concept = Concept(
                                    term=term,
                                    source_url=link_url,
                                    importance=importance,
                                    discovery_time=time.time()
                                )
                                concepts.append(concept)
                except:
                    pass
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(0.5)
        
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            self.failed_searches += 1
        
        return concepts
    
    def _simulate_search(self, keywords: List[str]) -> List[Concept]:
        """–°–∏–º—É–ª—è—Ü–∏—è –ø–æ–∏—Å–∫–∞ (–∫–æ–≥–¥–∞ requests –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)"""
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        simulated_concepts = [
            "neural architecture", "deep learning", "gradient descent",
            "backpropagation", "attention mechanism", "transformer model",
            "convolutional network", "recurrent network", "generative model",
            "reinforcement learning", "policy gradient", "value function",
            "embedding space", "latent representation", "feature extraction",
            "batch normalization", "dropout regularization", "weight decay",
            "learning rate", "optimizer algorithm", "loss function",
            "activation function", "softmax layer", "pooling operation"
        ]
        
        concepts = []
        for _ in range(random.randint(3, 8)):
            term = random.choice(simulated_concepts)
            if term not in [c.term for c in concepts]:
                concept = Concept(
                    term=term,
                    definition=f"Simulated concept related to {', '.join(keywords)}",
                    importance=random.uniform(0.3, 1.0),
                    discovery_time=time.time()
                )
                concepts.append(concept)
        
        return concepts
    
    def _add_concept(self, concept: Concept):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç–∞ –≤ –±–∞–∑—É"""
        if concept.term in self.concepts:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π
            existing = self.concepts[concept.term]
            existing.access_count += 1
            existing.importance = max(existing.importance, concept.importance)
            if concept.definition and not existing.definition:
                existing.definition = concept.definition
        else:
            self.concepts[concept.term] = concept
    
    async def search_async(self, keywords: Optional[List[str]] = None) -> List[Concept]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤"""
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤ executor –¥–ª—è –Ω–µ–±–ª–æ–∫–∏—Ä—É—é—â–µ–≥–æ –ø–æ–∏—Å–∫–∞
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.search_concepts, keywords)
    
    def get_concept(self, term: str) -> Optional[Concept]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç–∞ –ø–æ —Ç–µ—Ä–º–∏–Ω—É"""
        if term in self.concepts:
            self.concepts[term].access_count += 1
            return self.concepts[term]
        return None
    
    def get_related_concepts(self, term: str, top_k: int = 5) -> List[Concept]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤"""
        if term not in self.concepts:
            return []
        
        target = self.concepts[term]
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—é —Å–ª–æ–≤
        target_words = set(term.lower().split())
        
        related = []
        for other_term, other_concept in self.concepts.items():
            if other_term == term:
                continue
            
            other_words = set(other_term.lower().split())
            overlap = len(target_words & other_words)
            
            if overlap > 0:
                related.append((other_concept, overlap))
        
        related.sort(key=lambda x: x[1], reverse=True)
        return [c for c, _ in related[:top_k]]
    
    def get_top_concepts(self, top_k: int = 10) -> List[Concept]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–ø –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏"""
        sorted_concepts = sorted(
            self.concepts.values(),
            key=lambda c: c.importance * (1 + c.access_count * 0.1),
            reverse=True
        )
        return sorted_concepts[:top_k]
    
    def get_statistics(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–∏—Å–∫–∞"""
        return {
            'total_concepts': len(self.concepts),
            'total_searches': self.total_searches,
            'total_concepts_found': self.total_concepts_found,
            'failed_searches': self.failed_searches,
            'last_search_time': self.last_search_time,
            'search_interval': self.search_interval,
            'base_keywords': self.base_keywords
        }
    
    def save(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –±–∞–∑—ã –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤"""
        import json
        
        data = {
            'concepts': {k: v.to_dict() for k, v in self.concepts.items()},
            'statistics': self.get_statistics()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"üìö –ë–∞–∑–∞ –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filepath}")
    
    def load(self, filepath: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤"""
        import json
        
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for term, concept_data in data.get('concepts', {}).items():
            self.concepts[term] = Concept(**concept_data)
        
        print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.concepts)} –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –∏–∑ {filepath}")


class ConceptIntegrator:
    """
    –ò–Ω—Ç–µ–≥—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ü–µ–ø—Ç–æ–≤ –≤ –ø–∞–º—è—Ç—å —Å–∏—Å—Ç–µ–º—ã
    
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ç—ã –≤ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏
    –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –∏—Ö –≤ –Ω—É–∫–ª–µ–æ—Ç–∏–¥—ã/–≤–æ–∫—Å–µ–ª–∏.
    """
    
    def __init__(self, vector_size: int = 512):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ç–æ—Ä–∞
        
        Args:
            vector_size: —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
        """
        self.vector_size = vector_size
        
        # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self._char_to_idx = {chr(i): i - 32 for i in range(32, 127)}
    
    def concept_to_vector(self, concept: Concept) -> List[float]:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä
        
        Args:
            concept: –∫–æ–Ω—Ü–µ–ø—Ç
        
        Returns:
            –í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        """
        # –ü—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥: —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤
        text = f"{concept.term} {concept.definition}"
        
        vector = [0.0] * self.vector_size
        
        for i, char in enumerate(text[:self.vector_size]):
            idx = self._char_to_idx.get(char, 0)
            pos = i % self.vector_size
            vector[pos] += (idx / 95.0) * 0.1  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        import math
        for i in range(self.vector_size):
            vector[i] += math.sin(i / 10.0) * 0.05 * concept.importance
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        norm = sum(v * v for v in vector) ** 0.5
        if norm > 0:
            vector = [v / norm for v in vector]
        
        return vector
    
    def integrate_into_nucleotide(self, concept: Concept, nucleotide) -> None:
        """
        –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–∞ –≤ –Ω—É–∫–ª–µ–æ—Ç–∏–¥
        
        Args:
            concept: –∫–æ–Ω—Ü–µ–ø—Ç –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
            nucleotide: —Ü–µ–ª–µ–≤–æ–π –Ω—É–∫–ª–µ–æ—Ç–∏–¥
        """
        import numpy as np
        
        vector = self.concept_to_vector(concept)
        experience = np.array(vector, dtype=np.float16)
        
        # –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º –∫–∞–∫ –æ–ø—ã—Ç
        nucleotide.update(0.016, experience)
    
    def integrate_into_voxel(self, concept: Concept, voxel) -> None:
        """
        –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∫–æ–Ω—Ü–µ–ø—Ç–∞ –≤ –≤–æ–∫—Å–µ–ª—å
        
        Args:
            concept: –∫–æ–Ω—Ü–µ–ø—Ç
            voxel: —Ü–µ–ª–µ–≤–æ–π –≤–æ–∫—Å–µ–ª—å
        """
        import numpy as np
        
        vector = self.concept_to_vector(concept)
        experience = np.array(vector[:64], dtype=np.float32)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å –≤–æ–∫—Å–µ–ª—è
        voxel.memory.store(experience, importance=concept.importance)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –º—ã—Å–ª—å
        from .voxel import ThoughtType
        voxel.thoughts.add_thought(ThoughtType.OBSERVATION, experience)
